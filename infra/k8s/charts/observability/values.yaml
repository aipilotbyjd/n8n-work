# N8N Work Observability Stack Configuration

# Global settings
global:
  imageRegistry: ""
  storageClass: ""

# Prometheus Stack Configuration
prometheus:
  enabled: true

kube-prometheus-stack:
  # Prometheus Configuration
  prometheus:
    enabled: true
    prometheusSpec:
      retention: 30d
      retentionSize: 50GB
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: fast-ssd
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 100Gi
      
      resources:
        limits:
          cpu: 2000m
          memory: 4Gi
        requests:
          cpu: 1000m
          memory: 2Gi
      
      # Service Monitor configurations
      serviceMonitorSelectorNilUsesHelmValues: false
      serviceMonitorSelector: {}
      
      # Rule selector
      ruleSelectorNilUsesHelmValues: false
      ruleSelector: {}
      
      # Additional scrape configs for N8N Work services
      additionalScrapeConfigs:
        - job_name: 'n8n-work-orchestrator'
          kubernetes_sd_configs:
            - role: endpoints
              namespaces:
                names:
                  - n8n-work-dev
                  - n8n-work-staging
                  - n8n-work-prod
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_name]
              action: keep
              regex: orchestrator
            - source_labels: [__meta_kubernetes_endpoint_port_name]
              action: keep
              regex: metrics
        
        - job_name: 'n8n-work-engine-go'
          kubernetes_sd_configs:
            - role: endpoints
              namespaces:
                names:
                  - n8n-work-dev
                  - n8n-work-staging
                  - n8n-work-prod
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_name]
              action: keep
              regex: engine-go
            - source_labels: [__meta_kubernetes_endpoint_port_name]
              action: keep
              regex: metrics
        
        - job_name: 'n8n-work-node-runner'
          kubernetes_sd_configs:
            - role: endpoints
              namespaces:
                names:
                  - n8n-work-dev
                  - n8n-work-staging
                  - n8n-work-prod
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_name]
              action: keep
              regex: node-runner-js
            - source_labels: [__meta_kubernetes_endpoint_port_name]
              action: keep
              regex: metrics

  # Grafana Configuration
  grafana:
    enabled: true
    adminPassword: "admin123" # Change in production
    
    persistence:
      enabled: true
      size: 10Gi
      storageClassName: fast-ssd
    
    resources:
      limits:
        cpu: 500m
        memory: 1Gi
      requests:
        cpu: 250m
        memory: 512Mi
    
    # Additional data sources
    additionalDataSources:
      - name: Loki
        type: loki
        url: http://loki:3100
        access: proxy
        isDefault: false
        jsonData:
          httpHeaderName1: 'X-Scope-OrgID'
        secureJsonData:
          httpHeaderValue1: '1'
      
      - name: Jaeger
        type: jaeger
        url: http://jaeger-query:16686
        access: proxy
        isDefault: false
    
    # Dashboard providers
    dashboardProviders:
      dashboardproviders.yaml:
        apiVersion: 1
        providers:
          - name: 'default'
            orgId: 1
            folder: ''
            type: file
            disableDeletion: false
            editable: true
            options:
              path: /var/lib/grafana/dashboards/default
          - name: 'n8n-work'
            orgId: 1
            folder: 'N8N Work'
            type: file
            disableDeletion: false
            editable: true
            options:
              path: /var/lib/grafana/dashboards/n8n-work
    
    # N8N Work specific dashboards
    dashboards:
      n8n-work:
        workflow-overview:
          gnetId: 1860
          revision: 27
          datasource: Prometheus
        
        system-resources:
          gnetId: 3662
          revision: 2
          datasource: Prometheus
        
        database-monitoring:
          gnetId: 9628
          revision: 7
          datasource: Prometheus
    
    # Ingress configuration
    ingress:
      enabled: true
      ingressClassName: nginx
      annotations:
        nginx.ingress.kubernetes.io/ssl-redirect: "true"
        cert-manager.io/cluster-issuer: "letsencrypt-prod"
      hosts:
        - grafana.n8n-work.local
      tls:
        - secretName: grafana-tls
          hosts:
            - grafana.n8n-work.local

  # Alertmanager Configuration
  alertmanager:
    enabled: true
    alertmanagerSpec:
      storage:
        volumeClaimTemplate:
          spec:
            storageClassName: fast-ssd
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 10Gi
      
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      
      # Alertmanager configuration
      configSecret: alertmanager-config
    
    # Alertmanager configuration
    config:
      global:
        smtp_smarthost: 'localhost:587'
        smtp_from: 'alertmanager@n8n-work.com'
        smtp_auth_username: 'alertmanager@n8n-work.com'
        smtp_auth_password: 'password'
      
      route:
        group_by: ['alertname', 'cluster', 'service']
        group_wait: 10s
        group_interval: 10s
        repeat_interval: 1h
        receiver: 'default'
        routes:
          - match:
              severity: critical
            receiver: 'critical-alerts'
          - match:
              severity: warning
            receiver: 'warning-alerts'
      
      receivers:
        - name: 'default'
          email_configs:
            - to: 'team@n8n-work.com'
              subject: '[N8N Work] {{ .GroupLabels.alertname }}'
              body: |
                {{ range .Alerts }}
                Alert: {{ .Annotations.summary }}
                Description: {{ .Annotations.description }}
                {{ end }}
        
        - name: 'critical-alerts'
          email_configs:
            - to: 'critical@n8n-work.com'
              subject: '[CRITICAL] N8N Work Alert'
              body: |
                {{ range .Alerts }}
                CRITICAL ALERT: {{ .Annotations.summary }}
                Description: {{ .Annotations.description }}
                Labels: {{ .Labels }}
                {{ end }}
          webhook_configs:
            - url: 'http://alertmanager-webhook:9093/hooks/critical'
        
        - name: 'warning-alerts'
          email_configs:
            - to: 'warnings@n8n-work.com'
              subject: '[WARNING] N8N Work Alert'
              body: |
                {{ range .Alerts }}
                Warning: {{ .Annotations.summary }}
                Description: {{ .Annotations.description }}
                {{ end }}

# Loki Configuration (Log Aggregation)
loki:
  enabled: true
  
  loki:
    auth_enabled: false
    
    server:
      http_listen_port: 3100
    
    ingester:
      lifecycler:
        address: 127.0.0.1
        ring:
          kvstore:
            store: inmemory
          replication_factor: 1
        final_sleep: 0s
      chunk_idle_period: 1h
      max_chunk_age: 1h
      chunk_retain_period: 30s
    
    schema_config:
      configs:
        - from: 2020-10-24
          store: boltdb-shipper
          object_store: filesystem
          schema: v11
          index:
            prefix: index_
            period: 24h
    
    storage_config:
      boltdb_shipper:
        active_index_directory: /loki/boltdb-shipper-active
        cache_location: /loki/boltdb-shipper-cache
        shared_store: filesystem
      filesystem:
        directory: /loki/chunks
    
    limits_config:
      enforce_metric_name: false
      reject_old_samples: true
      reject_old_samples_max_age: 168h
      ingestion_rate_mb: 16
      ingestion_burst_size_mb: 32
    
    compactor:
      working_directory: /loki/boltdb-shipper-compactor
      shared_store: filesystem
  
  persistence:
    enabled: true
    size: 50Gi
    storageClassName: fast-ssd
  
  resources:
    limits:
      cpu: 1000m
      memory: 2Gi
    requests:
      cpu: 500m
      memory: 1Gi

# Promtail Configuration (Log Collection)
promtail:
  enabled: true
  
  config:
    clients:
      - url: http://loki:3100/loki/api/v1/push
    
    scrape_configs:
      - job_name: kubernetes-pods
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels:
              - __meta_kubernetes_pod_controller_name
            regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?
            action: replace
            target_label: __tmp_controller_name
          - source_labels:
              - __meta_kubernetes_pod_label_app_kubernetes_io_name
              - __meta_kubernetes_pod_label_app
              - __tmp_controller_name
              - __meta_kubernetes_pod_name
            regex: ^;*([^;]+)(;.*)?$
            action: replace
            target_label: app
          - source_labels:
              - __meta_kubernetes_pod_label_app_kubernetes_io_instance
              - __meta_kubernetes_pod_label_release
            regex: ^;*([^;]+)(;.*)?$
            action: replace
            target_label: instance
          - source_labels:
              - __meta_kubernetes_pod_label_app_kubernetes_io_component
              - __meta_kubernetes_pod_label_component
            regex: ^;*([^;]+)(;.*)?$
            action: replace
            target_label: component
          - action: replace
            source_labels:
              - __meta_kubernetes_pod_node_name
            target_label: node_name
          - action: replace
            source_labels:
              - __meta_kubernetes_namespace
            target_label: namespace
          - action: replace
            replacement: $1
            separator: /
            source_labels:
              - namespace
              - app
            target_label: job
          - action: replace
            source_labels:
              - __meta_kubernetes_pod_name
            target_label: pod
          - action: replace
            source_labels:
              - __meta_kubernetes_pod_container_name
            target_label: container
          - action: replace
            replacement: /var/log/pods/*$1/*.log
            separator: /
            source_labels:
              - __meta_kubernetes_pod_uid
              - __meta_kubernetes_pod_container_name
            target_label: __path__
          - action: replace
            regex: true/(.*)
            replacement: /var/log/pods/*$1/*.log
            separator: /
            source_labels:
              - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash
              - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash
              - __meta_kubernetes_pod_container_name
            target_label: __path__
        
        pipeline_stages:
          - match:
              selector: '{app="orchestrator"}'
              stages:
                - json:
                    expressions:
                      level: level
                      message: message
                      timestamp: timestamp
                - timestamp:
                    source: timestamp
                    format: RFC3339
                - labels:
                    level:
          
          - match:
              selector: '{app="engine-go"}'
              stages:
                - regex:
                    expression: '(?P<timestamp>\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}.\d{3}Z)\s+(?P<level>\w+)\s+(?P<message>.*)'
                - timestamp:
                    source: timestamp
                    format: RFC3339
                - labels:
                    level:
          
          - match:
              selector: '{app="node-runner-js"}'
              stages:
                - json:
                    expressions:
                      level: level
                      message: msg
                      timestamp: time
                - timestamp:
                    source: timestamp
                    format: RFC3339
                - labels:
                    level:

# Jaeger Configuration (Distributed Tracing)
jaeger:
  enabled: true
  
  storage:
    type: elasticsearch
    elasticsearch:
      host: elasticsearch
      port: 9200
      user: elastic
      password: changeme
  
  query:
    replicaCount: 1
    resources:
      limits:
        cpu: 500m
        memory: 1Gi
      requests:
        cpu: 250m
        memory: 512Mi
    
    ingress:
      enabled: true
      ingressClassName: nginx
      annotations:
        nginx.ingress.kubernetes.io/ssl-redirect: "true"
        cert-manager.io/cluster-issuer: "letsencrypt-prod"
      hosts:
        - jaeger.n8n-work.local
      tls:
        - secretName: jaeger-tls
          hosts:
            - jaeger.n8n-work.local
  
  collector:
    replicaCount: 1
    resources:
      limits:
        cpu: 500m
        memory: 1Gi
      requests:
        cpu: 250m
        memory: 512Mi
  
  agent:
    resources:
      limits:
        cpu: 200m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 128Mi

# Custom ServiceMonitors for N8N Work components
serviceMonitors:
  orchestrator:
    enabled: true
    labels:
      app.kubernetes.io/name: orchestrator
    endpoints:
      - port: metrics
        path: /metrics
        interval: 30s
  
  engineGo:
    enabled: true
    labels:
      app.kubernetes.io/name: engine-go
    endpoints:
      - port: metrics
        path: /metrics
        interval: 30s
  
  nodeRunner:
    enabled: true
    labels:
      app.kubernetes.io/name: node-runner-js
    endpoints:
      - port: metrics
        path: /metrics
        interval: 30s

# Custom PrometheusRules for N8N Work alerts
prometheusRules:
  enabled: true
  groups:
    - name: n8n-work.rules
      rules:
        # Workflow execution alerts
        - alert: WorkflowExecutionFailureRate
          expr: (rate(n8n_workflow_executions_failed_total[5m]) / rate(n8n_workflow_executions_total[5m])) > 0.1
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "High workflow execution failure rate"
            description: "Workflow execution failure rate is {{ $value | humanizePercentage }} over the last 5 minutes"
        
        - alert: WorkflowExecutionStalled
          expr: increase(n8n_workflow_executions_total[5m]) == 0
          for: 10m
          labels:
            severity: critical
          annotations:
            summary: "No workflow executions in the last 10 minutes"
            description: "There have been no workflow executions for 10 minutes, which may indicate a system issue"
        
        # Resource usage alerts
        - alert: HighCPUUsage
          expr: (rate(container_cpu_usage_seconds_total{pod=~"orchestrator-.*|engine-go-.*|node-runner-js-.*"}[5m]) * 100) > 80
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High CPU usage detected"
            description: "Pod {{ $labels.pod }} CPU usage is {{ $value }}%"
        
        - alert: HighMemoryUsage
          expr: (container_memory_working_set_bytes{pod=~"orchestrator-.*|engine-go-.*|node-runner-js-.*"} / container_spec_memory_limit_bytes) * 100 > 85
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High memory usage detected"
            description: "Pod {{ $labels.pod }} memory usage is {{ $value }}%"
        
        # Database alerts
        - alert: DatabaseConnectionFailure
          expr: up{job="postgresql"} == 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "Database connection failure"
            description: "PostgreSQL database is not responding"
        
        # Message queue alerts
        - alert: RabbitMQConnectionFailure
          expr: up{job="rabbitmq"} == 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "RabbitMQ connection failure"
            description: "RabbitMQ is not responding"
        
        - alert: RabbitMQHighQueueLength
          expr: rabbitmq_queue_messages > 1000
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "RabbitMQ queue length is high"
            description: "Queue {{ $labels.queue }} has {{ $value }} messages"
        
        # Node runner security alerts
        - alert: SandboxSecurityViolation
          expr: increase(n8n_sandbox_security_violations_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: "Sandbox security violation detected"
            description: "Security violation detected in sandbox: {{ $labels.violation_type }}"
        
        # Disk space alerts
        - alert: DiskSpaceUsage
          expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 10
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Low disk space"
            description: "Disk space usage on {{ $labels.instance }} is {{ $value }}%"